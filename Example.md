## Basic concepts ##

Let's glance through this
<a href='http://jahmm.googlecode.com/svn/trunk/src/main/java/be/ac/ulg/montefiore/run/jahmm/apps/sample/SimpleExample.java'>example source file</a>
to see how it's been written. Its aim is to:

  * build a HMM with given parameters;
  * generate sequences of observations using this HMM;
  * learn the parameters of a HMM using those sequences;
  * compute the distance between the original HMM and the learnt HMM (the smaller, the better).

A HMM is a common way to model the probabilities of receiving a corrupted packet in a wireless network. It supposes that the wireless channel is either jammed or not (e.g. because of some kind of interference); this status modifies the probability of packet loss.

First, let's build an HMM that generates discrete observations, depicted below (with obvious notations):
![http://jahmm.googlecode.com/svn/wiki/images/example/exampleHmm.png](http://jahmm.googlecode.com/svn/wiki/images/example/exampleHmm.png)


The following program builds this HMM:

```
OpdfDiscreteFactory<Packet> factory =
  new OpdfDiscreteFactory<Packet>(Packet.class);
Hmm<ObservationDiscrete<Packet>> hmm = 
  new Hmm<ObservationDiscrete<Packet>>(2, factory);

hmm.setPi(0, 0.95);
hmm.setPi(1, 0.05);

hmm.setOpdf(0, new OpdfDiscrete<Packet>(Packet.class, new double[] { 0.95, 0.05 }));
hmm.setOpdf(1, new OpdfDiscrete<Packet>(Packet.class, new double[] { 0.2, 0.8 }));

hmm.setAij(0, 1, 0.05);
hmm.setAij(0, 0, 0.95);
hmm.setAij(1, 0, 0.1);
hmm.setAij(1, 1, 0.9);
```

This is straightforward:

  * Build a new HMM with 2 states and dealing with discrete observations whose values are those of the Packet enumeration (see the begining of the program to see the definition of Packet). The factory object initialise the observation distributions of each state to a discrete distribution.
  * Set the pi value of each state (i.e. the probability that the state is initial).
  * Build new distributions on discrete observations and associate them to each state.
  * Set each Ai,j value.

Notice that one could have used integer values instead of discrete one. In this case, the value '0' (resp. '1') would have been associated to 'no packet loss' (resp. 'a packet loss'). The equivalent program would have been written:

```
OpdfIntegerFactory factory = new OpdfIntegerFactory(2);
Hmm<ObservationInteger> hmm = new Hmm<ObservationInteger>(2, factory);
	
hmm.setPi(0, 0.95);
hmm.setPi(1, 0.05);

hmm.setOpdf(0, new OpdfInteger(new double[] { 0.95, 0.05 }));
hmm.setOpdf(1, new OpdfInteger(new double[] { 0.2, 0.8 }));
...
```

The argument ('2') of the `OpdfIntegerFactory` object constructor means that the observations can only have two values ('0' and '1').

Now, let's build 200 observation sequences (of length 100) based on this HMM:

```
static <O extends Observation> List<List<O>> generateSequences(Hmm<O> hmm)
{
  MarkovGenerator<O> mg = new MarkovGenerator<O>(hmm);
  List<List<O>> sequences = new ArrayList<List<O>>();

  for (int i = 0; i < 200; i++)
    sequences.add(mg.observationSequence(100));

  return sequences;
}
```

This function uses generics: `<O>` corresponds to a class of observations compatible with the HMM used to generate the sequences (in this case, `ObservationDiscrete<Packet>`). Notice that 200 sequences made of 100 observations is not equivalent to a sequence of 20000 observations: having a large number of sequences helps estimating the initial state probability values. The return value of this function is thus a set of observation sequences.

Now we can build a BaumWelchLearner object that can find an HMM fitted to the observation sequences we've just generated:

```
BaumWelchLearner bwl = new BaumWelchLearner();
Hmm<?> learntHmm = bwl.learn(hmm, sequences);
```

This applies a few iterations of the Baum-Welch algorithm. An initial HMM is given as an argument, together with the observation sequences generated by the HMM that we try to model. The method iterate only applies one iteration of the algorithm.

The function buildInitHmm() builds an HMM that is similar to the original one, but with slightly modified values; the Baum-Welch algorithm only finds a local minima of its optimum function, so an initial approximation of the result is needed. Giving a random HMM would not work! After that, learning iterations are performed, and the result is put in learntHmm.



## Other possibilities ##

Now let's see how the Baum-Welch algorithm converge. We can see the distance between the HMM generated after each iteration and the "original" HMM (i.e. the one that has been used to generate the learning sequences).

```
KullbackLeiblerDistanceCalculator klc = new KullbackLeiblerDistanceCalculator();
     
for (int i = 0; i < 10; i++) {
  System.out.println(i + " " + klc.distance(learntHmm, hmm));
  learntHmm = bwl.iterate(learntHmm);
}
```

The idea is to generate 10 iterations, and at each iteration to compute the Kullback-Leibler distance (see the javadoc of `KullbackLeiblerDistanceCalculator` to obtain more information about this probabilistic distance measure) between the learnt HMM and the original one. The result is the following:
```
0 0.09947315997789827
1 0.01583685086190477
2 0.005198400493981706
3 0.004084742535277281
4 5.750836951005057E-4
5 3.7337016867911076E-4
6 7.369629290423065E-6
7 2.1984953143516463E-4
8 -7.690338464425394E-5
9 -2.6738212518938564E-5
```

This can directly be fed to a plotting program (here gnuplot script) to generate:
![http://jahmm.googlecode.com/svn/wiki/images/example/hmmExample.png](http://jahmm.googlecode.com/svn/wiki/images/example/hmmExample.png)

It's always interesting to have a graphical output of what's going on. Let's draw the resulting learnt HMM to see how it differs from the original one:

```
(new GenericHmmDrawerDot()).write(learntHmm, "learntHmm.dot");
```

The arguments of the write method are of course the HMM to draw and the file name of the resulting dot output. This file can be converted to PostScript using the dot tool:

<tt>dot -Tps learntHmm.dot -o learntHmm.ps</tt>

This gives this result:
![http://jahmm.googlecode.com/svn/wiki/images/example/learntHmm.png](http://jahmm.googlecode.com/svn/wiki/images/example/learntHmm.png)

Each state is depicted by a circle. In each circle, the first number is the state number. The initial probability Pi is given if it is higher than a small threshold (the state is then drawn with a double line). The numbers in the brackets give the probabilities of each observation.

The small differences with the original HMM, depicted at the top of this page, are because:

  * Only 200 sequences have been used, so the estimation of the initial state can't be done properly;
  * The learning sequences are quite short.



## Data files ##

An observation sequence or a set of observation sequences can be read from a file.

Each sequence line holds a sequence and is composed of observations separated by a semi-colon. For example, the following file holds two sequences of integers:

```
# A simple data file

1; 2; 3; 
2; 4; 6;
```

This other example holds two sequences of vectors; each vector has a dimension of 2. The first observation of the second sequence is the vector (2., 3.).

```
# A simple data file

[1. 2.]; [2. 3.]; [3. 4.]; 
[2. 3.]; [3. 4.]; [4. 5.]; 
```

A sequence can span on multiple lines using lines terminated by a backslash.

A simple program such as this one can read this last example:

```
Reader reader = new FileReader("vectors.seq");
List<List<ObservationVector>> v = ObservationSequencesReader.
  readSequences(new ObservationVectorReader(2), reader);
reader.close();
```

The (optional) argument of the constructor of `jahmm.io.ObservationVectorReader` asks to verify the dimension of the vectors read (which must be 2 here). This class must be replaced by `jahmm.io.ObservationIntegerReader` if the observations read are integers. After the execution of this code, the vector v holds the observation sequences held in the file named vectors.seq; those sequences can be directly feeded to a learning algorithm. This code can throw `java.io.IOException` (if an I/O error occurs) or a `jahmm.io.FileFormatException` (if the file is syntactically incorrect).

I hope this gives a good overview of the possibilities of this HMM implementation.

Have fun!